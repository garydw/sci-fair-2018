{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### constants\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BN_MOMENTUM = 0.9997\n",
    "EPSILON = 1e-4\n",
    "\n",
    "### utils\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        _dict = pickle.load(f, encoding='bytes')\n",
    "    return _dict\n",
    "\n",
    "def get_v(name, shape, initializer=tf.variance_scaling_initializer, decay=None, trainable=True, dtype='float'):\n",
    "    regularizer = None\n",
    "    if decay:\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(decay)\n",
    "    return tf.get_variable(name, shape=shape, initializer=initializer, regularizer=regularizer, trainable=trainable, dtype=dtype)\n",
    "\n",
    "def conv(x, out, ksize=3, stride=1):\n",
    "    weights = get_v('weights', [ksize, ksize, x.get_shape()[-1], out], decay=WEIGHT_DECAY)\n",
    "    return tf.nn.conv2d(x, weights, [1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def fc(x, units, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        weights = get_v('weights', [x.get_shape()[1], units], decay=WEIGHT_DECAY)\n",
    "        biases = get_v('biases', [units], initializer=tf.zeros_initializer)\n",
    "        x = tf.nn.xw_plus_b(x, weights, biases)\n",
    "    return x\n",
    "\n",
    "def bn(x, training):\n",
    "    return tf.layers.batch_normalization(x, momentum=BN_MOMENTUM, epsilon=EPSILON, training=training)\n",
    "\n",
    "def activation(x, typ='relu'):\n",
    "    if typ == 'relu':\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "def block(x, training, bottleneck=False, downsample=False):\n",
    "    identity = x\n",
    "    stride = 2 if downsample else 1\n",
    "    out = x.get_shape()[-1] * 2 if downsample else x.get_shape()[-1]\n",
    "    if bottleneck:\n",
    "        internal = x.get_shape()[-1] / 2 if downsample else x.get_shape()[-1] / 4\n",
    "        with tf.variable_scope('bottle_1'):\n",
    "            x = conv(x, internal, ksize=1, stride=stride) \n",
    "            x = bn(x, training)\n",
    "            x = activation(x)\n",
    "        with tf.variable_scope('bottle_2'):\n",
    "            x = conv(x, internal)\n",
    "            x = bn(x, training)\n",
    "            x = activation(x)\n",
    "        with tf.variable_scope('bottle_3'):\n",
    "            x = conv(x, internal * 4, ksize=1)\n",
    "            x = bn(x, training)\n",
    "    else:\n",
    "        with tf.variable_scope('reg_1'):\n",
    "            x = conv(x, out, stride=stride)\n",
    "            x = bn(x, training)\n",
    "            x = activation(x)\n",
    "        with tf.variable_scope('reg_2'):\n",
    "            x = conv(x, out)\n",
    "            x = bn(x, training)\n",
    "    with tf.variable_scope('identity'):\n",
    "        if downsample:\n",
    "            in_channels = identity.get_shape()[-1]\n",
    "            identity = tf.nn.avg_pool(identity, ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "            channels = int((int(x.get_shape()[-1]) - int(in_channels)) / 2)\n",
    "            #double_zero = tf.constant([0, 0], tf.int64)\n",
    "            #chan = tf.pack([channels, channels])\n",
    "            #padding = tf.pack([double_zero * 3, chan])\n",
    "            padding = [[0, 0], [0, 0], [0, 0], [channels, channels]]\n",
    "            identity = tf.pad(identity, padding)\n",
    "        x = activation(x + identity)\n",
    "    return x\n",
    "\n",
    "def group(x, nblock, scope, bottleneck=False, training=True, downsample=True):\n",
    "    with tf.variable_scope(scope):\n",
    "        for i in range(nblock):\n",
    "            if downsample:\n",
    "                downsample = True if i == 0 else False\n",
    "            else:\n",
    "                downsample = False if i > 0 else True\n",
    "            with tf.variable_scope('block{}'.format(i + 1)):\n",
    "                x = block(x, training, bottleneck=bottleneck, downsample=downsample)\n",
    "    return x        \n",
    "    \n",
    "def flatten(x):\n",
    "    a, b, c = x.get_shape()[1:]\n",
    "    x = tf.reshape(x, [-1, a * b * c])\n",
    "    return x\n",
    "\n",
    "def prepare_cifar(directory):\n",
    "    cifar = []\n",
    "    for num in range(5):\n",
    "        cifar.append(unpickle(directory + '\\\\data_batch_{}'.format(num + 1)))\n",
    "    test = unpickle(directory + '\\\\test_batch')\n",
    "    unshaped = np.stack([cifar[x][b'data'] for x in range(len(cifar))]).reshape(-1, 3, 32, 32)\n",
    "    data = np.empty((unshaped.shape[0], 32, 32, 3))\n",
    "    train_labels = np.stack([cifar[x][b'labels'] for x in range(len(cifar))]).reshape(50000)\n",
    "    test_data_unshaped = test[b'data'].reshape(-1, 3, 32, 32)\n",
    "    test_data = np.empty((test[b'data'].shape[0], 32, 32, 3))\n",
    "    for i in range(data.shape[3]):\n",
    "        data[:, :, :, i] = unshaped[:, i, :, :]\n",
    "        test_data[:, :, :, i] = test_data_unshaped[:, i, :, :]\n",
    "    test_labels = np.array(test[b'labels'])\n",
    "    return (data, train_labels), (test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984990 Train accuracy: 0.19 Test accuracy 0.2511\n",
      "01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984991 Train accuracy: 0.32 Test accuracy 0.3192\n",
      "01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984992 Train accuracy: 0.42 Test accuracy 0.3776\n",
      "012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-a8b3c98eba89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0msummary_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxentropy_summary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matthew\\documents\\sciencefair2017-2018\\pythoncode\\neuralnet\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matthew\\documents\\sciencefair2017-2018\\pythoncode\\neuralnet\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matthew\\documents\\sciencefair2017-2018\\pythoncode\\neuralnet\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matthew\\documents\\sciencefair2017-2018\\pythoncode\\neuralnet\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matthew\\documents\\sciencefair2017-2018\\pythoncode\\neuralnet\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### resnet32 construction\n",
    "decay = .01 \n",
    "n = 5\n",
    "X = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "training = tf.placeholder(tf.bool, name='training')\n",
    "with tf.name_scope('cnn'):\n",
    "    with tf.variable_scope('conv'):\n",
    "        out = conv(X, 16)\n",
    "    out = group(out, n, 'group1', downsample=False)\n",
    "    out = group(out, n, 'group2')\n",
    "    out = group(out, n, 'group3')\n",
    "    out = flatten(out)\n",
    "    logits = fc(out, 10, 'fc')\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy)\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name='loss')\n",
    "learning_rate = 0.1\n",
    "with tf.name_scope('train'):\n",
    "    optimizer= tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = 'tf_logs'\n",
    "logdir = '{}/run-{}/'.format(root_logdir, now)\n",
    "xentropy_summary = tf.summary.scalar('xentropy', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "(train_data, train_labels), (test_data, test_labels) = prepare_cifar(r'C:\\Users\\matthew\\Documents\\ScienceFair2017-2018\\PythonCode\\NeuralNet\\env\\cifar-10_test')\n",
    "with tf.Session() as sess:\n",
    "    init.run()(\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(50000 // batch_size):\n",
    "            index = iteration * batch_size\n",
    "            X_batch = train_data[index:index + batch_size, :, :, :]\n",
    "            y_batch = train_labels[index:index + batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            if iteration % 10 == 0:\n",
    "                summary_str = xentropy_summary.eval(feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "                step = epoch * (50000 // batch_size) + iteration\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            print(iteration, end='', flush=True)\n",
    "        learning_rate = learning_rate * 1/1+decay*epoch)\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch, training: False})\n",
    "        acc_test = accuracy.eval(feed_dict={X: test_data, y: test_labels, training: False})\n",
    "        print(epoch, 'Train accuracy:', acc_train, 'Test accuracy', acc_test)\n",
    "    save_path = saver.save(sess, './resnet_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(testing_data, test_labels), (testing_test_data, testing_test_labels) = prepare_cifar(r'C:\\Users\\matthew\\Documents\\ScienceFair2017-2018\\PythonCode\\NeuralNet\\cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
